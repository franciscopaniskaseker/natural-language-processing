{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58571aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy library\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a896a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f920f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# print the name of the pipeline components\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03eaf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x11753c5e0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x124229d60>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x1241415f0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x124430b40>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x12443f9c0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x1241417b0>)]\n"
     ]
    }
   ],
   "source": [
    "# print the full pipeline name\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49dd3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets create a pipeline. first, import the requeried library\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb33a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then define a custom pipeline\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5266bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.custom_component_function(doc)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and finally add this pipeline to the spacy object, before the \"ner\" pipeline\n",
    "nlp.add_pipe(\"custom_component\", before=\"ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8898fde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'custom_component', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Then check again all the pipelines names:\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85102a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 10\n"
     ]
    }
   ],
   "source": [
    "# Lets tokenize some text. When the custom_component pipele is executed, we will see the lenght of th doc\n",
    "doc = nlp(\"Hello word! Today we will do our best!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b58be138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hello, word, !, Today, we, will, do, our, best, !]\n"
     ]
    }
   ],
   "source": [
    "# just to remember you, there are 10 tokens in the doc:\n",
    "list_tokens = []\n",
    "for token in doc:\n",
    "    list_tokens.append(token)\n",
    "\n",
    "print(list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711eb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets for animals in the some document and\n",
    "# - add the matched Spans, using PhraseMatcher (match parreterns in the for of Doc objects\n",
    "# - and add them in the Docs.ents (know entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6691e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed libraries\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669544e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a new spacy vocab\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5673e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text that will be processed\n",
    "doc = nlp(\"I have a cat and a Golden Retriever. I do not have a Snake or an Elephant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9565763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Golden Retriever', 'cat', 'turtle', 'Rattus norvegicus', 'Elephant']\n"
     ]
    }
   ],
   "source": [
    "# a list of animals to be found\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\", \"Elephant\"]\n",
    "print(animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f90a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus, Elephant]\n"
     ]
    }
   ],
   "source": [
    "# tokenize them and put inside of list \n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bebd9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add to the matcher the new patterns\n",
    "# first, create the PhraseMatcher object using the shared vocabulary\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "# and then add the animal partterns\n",
    "matcher.add(\"ANIMAL\", animal_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2162e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1e69e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before continue, lets show what is inside of every object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f7a0ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.matcher.phrasematcher.PhraseMatcher object at 0x1245265f0>\n"
     ]
    }
   ],
   "source": [
    "# matcher object is a PhraseMatcher object type that is using the nlp.vocab;\n",
    "# And nlp is a trained spacy pipeline called en_core_web_sm \n",
    "print(matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27a88789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6303828839600189595, 3, 4), (6303828839600189595, 6, 8), (6303828839600189595, 17, 18)]\n"
     ]
    }
   ],
   "source": [
    "# Looks into the doc for the animmal patterns. 3 were found.\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "# The list contain 3 items. Every item has 3 informations: the match_id, where it start and where it ends\n",
    "# the match_id is the hash_value of the string that was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbe01875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we will recreate the doc.ents based just in animal_patterns\n",
    "# We will create one Span for every match in matches list\n",
    "# Remembering that Span is a slice of the Doc object\n",
    "spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "# Overwrite the doc.ents with the matched spans\n",
    "doc.ents = spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27475b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cat, Golden Retriever, Elephant)\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6eadd505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.animal_component_function(doc)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets add the animal_component_function after the \"ner\" pipeline component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf1f4489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL'), ('Elephant', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "# And now lets use just the spacy pipelines to get the animals found and their label\n",
    "doc = nlp(\"I have a cat and a Golden Retriever. I do not have a Snake or an Elephant!\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89e20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
